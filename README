LATENT DIRICHLET ALLOCATION WITH TOPIC-IN-SET KNOWLEDGE (z-label LDA)
Version 0.1

David Andrzejewski (andrzeje@cs.wisc.edu)
Department of Computer Sciences
University of Wisconsin-Madison, USA

This software implements an extension of LDA [2] which allows the use
of "topic-in-set knowledge", or z-labels [1].  This allows the user to
supply (possibly noisy) labels or set-labels for specific latent topic
z-values. The inference method is Collapsed Gibbs sampling [3].  This
code can also be used to do "standard" LDA, similar to [3] (see
example code).

The model is implemented as a Python C extension module.


BUILD/INSTALL

Building this module requires installation of Python and NumPy, as
well as a C compiler.  From the command-line, do:

% python setup.py install

(Note that if NumPy or Python are installed to non-standard locations,
you may need to make the appropriate changes in setup.py)

You can then test the installation with:

% python test/testZLabelLDA.py -v

There is also a simple example scipt showing how to use zhLDA:

% python example.py


LOCAL INSTALL

If you do not have write access to your Python installation directories,
you will need to tell setup.py to install this module somewhere else.
For example:

% python setup.py install --prefix=~/local

will install the module under a subdirectory of your home directory called 
"local".

It may then be necessary to let Python know where that is by setting
the PYTHONPATH environment variable (e.g., in .bashrc or .cshrc).  For
our example this might involve adding something like the line:

setenv PYTHONPATH ~/local/lib/python2.5/site-packages


HOW TO USE

The commenting in the example.py script explains the meanings and
types of all input and return arguments.  For more detailed examples
of good/bad inputs, you can examine the tests in test/testZLabelLDA.py


LICENSE

This software is open-source, released under the terms of the GNU
General Public License version 3, or any later version of the GPL (see
LICENSE.txt).


REFERENCES

[1] Andrzejewski, D. and Zhu, X. (2009).  Latent Dirichlet Allocation
with Topic-in-Set Knowledge NAACL 2009 Workshop on Semi-supervised
Learning for NLP (NAACL-SSLNLP 2009)

[2] Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent dirichlet
allocation.  Journal of Machine Learning Research (JMLR) 3
(Mar. 2003), 993-1022.

[3] Griffiths, T., and Steyvers, M. (2004).  Finding Scientific
Topics. Proceedings of the National Academy of Sciences (PNAS), 101
(suppl. 1), 5228-5235.


VERSION HISTORY
0.1     Initial release
0.1.1   Reversed meaning of 'eta' parameter to be consistent with SSLNLP paper
0.1.2   Fixed example.py to remove DeltaLDA usage and use z-labels more clearly
